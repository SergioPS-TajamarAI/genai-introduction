{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abae97ae",
   "metadata": {},
   "source": [
    "# Limpieza Contextual de Texto\n",
    "Esta funci√≥n procesa texto crudo para preservar informaci√≥n cr√≠tica para an√°lisis comercial. Elimina elementos como URLs y handles de usuario, mientras preserva fechas, horas, precios, porcentajes, hashtags y emoticones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2d698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import emoji\n",
    "\n",
    "def clean_business_text(text):\n",
    "    \"\"\"\n",
    "    Procesa texto crudo preservando informaci√≥n cr√≠tica para an√°lisis comercial.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto a procesar\n",
    "        \n",
    "    Returns:\n",
    "        str: Texto procesado que preserva informaci√≥n comercial relevante\n",
    "    \"\"\"\n",
    "    # Paso 1: Eliminar URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Paso 2: Eliminar handles de usuario (@usuario)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Paso 3: Identificar patrones a preservar\n",
    "    # Buscamos coincidencias para fechas, horas, precios, porcentajes, etc.\n",
    "    preserved_patterns = []\n",
    "    \n",
    "    # Fechas (DD/MM/YYYY)\n",
    "    date_matches = re.finditer(r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b', text)\n",
    "    preserved_patterns.extend([(m.start(), m.end(), m.group(0)) for m in date_matches])\n",
    "    \n",
    "    # Horas (HH:MM)\n",
    "    time_matches = re.finditer(r'\\b\\d{1,2}:\\d{2}\\b', text)\n",
    "    preserved_patterns.extend([(m.start(), m.end(), m.group(0)) for m in time_matches])\n",
    "    \n",
    "    # Formatos comerciales (2x, $99.99, 15%)\n",
    "    format_matches = re.finditer(r'(\\b\\d+x\\b|\\$\\d+(\\.\\d+)?|\\d+(\\.\\d+)?%)', text)\n",
    "    preserved_patterns.extend([(m.start(), m.end(), m.group(0)) for m in format_matches])\n",
    "    \n",
    "    # Hashtags (#ejemplo)\n",
    "    hashtag_matches = re.finditer(r'#\\w+', text)\n",
    "    preserved_patterns.extend([(m.start(), m.end(), m.group(0)) for m in hashtag_matches])\n",
    "    \n",
    "    # Paso 4: Crear texto limpio \n",
    "    result = []\n",
    "    last_end = 0\n",
    "    \n",
    "    # Ordenar patrones a preservar por posici√≥n\n",
    "    preserved_patterns.sort()\n",
    "    \n",
    "    for start, end, pattern in preserved_patterns:\n",
    "        # Procesar texto entre patrones a preservar\n",
    "        segment = text[last_end:start]\n",
    "        # Limpiar segmento (eliminar caracteres no deseados)\n",
    "        segment = re.sub(r'[^\\w\\s!?%$/]', ' ', segment)\n",
    "        result.append(segment)\n",
    "        # A√±adir patr√≥n preservado\n",
    "        result.append(pattern)\n",
    "        last_end = end\n",
    "    \n",
    "    # Procesar texto despu√©s del √∫ltimo patr√≥n\n",
    "    if last_end < len(text):\n",
    "        segment = text[last_end:]\n",
    "        segment = re.sub(r'[^\\w\\s!?%$/]', ' ', segment)\n",
    "        result.append(segment)\n",
    "    \n",
    "    # Paso 5: Normalizar espacios y preservar emojis\n",
    "    cleaned_text = ''.join(result)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8462b029",
   "metadata": {},
   "source": [
    "# Normalizaci√≥n de N√∫meros y Unidades\n",
    "Esta funci√≥n normaliza n√∫meros y unidades en el texto, convirtiendo fechas a formato ISO, procesando monedas y reemplazando n√∫meros gen√©ricos con etiquetas como `<NUM>`. Tambi√©n convierte unidades como `2x` a `2_unidades`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb39d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateparser\n",
    "\n",
    "def normalize_numbers_and_units(text):\n",
    "    \"\"\"\n",
    "    Normaliza n√∫meros y unidades en el texto, preservando formatos clave para modelos de precios.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto a normalizar\n",
    "        \n",
    "    Returns:\n",
    "        str: Texto con n√∫meros y unidades normalizados\n",
    "    \"\"\"\n",
    "    # Paso 1: Normalizar fechas a formato ISO\n",
    "    date_pattern = r'\\b(\\d{1,2}[/\\-\\.]\\d{1,2}[/\\-\\.]\\d{2,4})\\b|\\b(\\d{1,2}\\s+de\\s+[a-zA-Z]+\\s+de\\s+\\d{2,4})\\b|\\b([a-zA-Z]+\\s+\\d{1,2},?\\s+\\d{2,4})\\b'\n",
    "    \n",
    "    def convert_date(match):\n",
    "        date_str = match.group(0)\n",
    "        try:\n",
    "            parsed_date = dateparser.parse(date_str, languages=['es', 'en'])\n",
    "            if parsed_date:\n",
    "                return parsed_date.strftime('%Y-%m-%d')\n",
    "            return date_str\n",
    "        except:\n",
    "            return date_str\n",
    "    \n",
    "    text = re.sub(date_pattern, convert_date, text)\n",
    "    \n",
    "    # Paso 2: Normalizar unidades de venta\n",
    "    unit_pattern = r'\\b(\\d+)(x|kg|g|ml|l)\\b'\n",
    "    \n",
    "    def normalize_unit(match):\n",
    "        number = match.group(1)\n",
    "        unit = match.group(2)\n",
    "        if unit == 'x':\n",
    "            return f\"{number}_unidades\"\n",
    "        else:\n",
    "            return f\"{number}_{unit}\"\n",
    "    \n",
    "    text = re.sub(unit_pattern, normalize_unit, text)\n",
    "    \n",
    "    # Paso 3: Procesar monedas\n",
    "    # USD: $XXX.XX -> <USD>XXX.XX\n",
    "    text = re.sub(r'\\$\\s*(\\d+(?:\\.\\d+)?)', r'<USD>\\1', text)\n",
    "    \n",
    "    # EUR: XXX.XX‚Ç¨ -> <EUR>XXX.XX\n",
    "    text = re.sub(r'(\\d+(?:\\.\\d+)?)\\s*(?:‚Ç¨|EUR)', r'<EUR>\\1', text)\n",
    "    \n",
    "    # Formato latinoamericano: XXX.XX$ -> <USD>XXX.XX\n",
    "    text = re.sub(r'(\\d+(?:\\.\\d+)?)\\s*\\$', r'<USD>\\1', text)\n",
    "    \n",
    "    # Paso 4: Reemplazar n√∫meros gen√©ricos con <NUM>\n",
    "    # Evitar reemplazar n√∫meros en formatos ya procesados\n",
    "    # Dividimos el lookbehind complejo en varios lookbehinds de ancho fijo\n",
    "    try:\n",
    "        standalone_number_pattern = r'(?<!\\d-)(?<!\\d/)(?<!\\d\\.)(?<!\\d_)(?<![A-Z]>)(?<!\\d)\\b\\d+(?:\\.\\d+)?\\b(?![-/.]\\d|_\\w|%|-\\d|-\\w)'\n",
    "        text = re.sub(standalone_number_pattern, '<NUM>', text)\n",
    "    except re.error:\n",
    "        # Si falla, usamos un m√©todo alternativo con marcado y restauraci√≥n\n",
    "        # Marcar n√∫meros que queremos preservar\n",
    "        text = re.sub(r'(\\d+)-(\\d+)', r'PRESERVE_\\1-PRESERVE_\\2', text)\n",
    "        text = re.sub(r'(\\d+)/(\\d+)', r'PRESERVE_\\1/PRESERVE_\\2', text)\n",
    "        text = re.sub(r'(\\d+)\\.(\\d+)', r'PRESERVE_\\1.PRESERVE_\\2', text)\n",
    "        text = re.sub(r'<(USD|EUR)>(\\d+(?:\\.\\d+)?)', r'<\\1>PRESERVE_\\2', text)\n",
    "        text = re.sub(r'(\\d+)_(\\w+)', r'PRESERVE_\\1_\\2', text)\n",
    "        text = re.sub(r'(\\d+)%', r'PRESERVE_\\1%', text)\n",
    "        \n",
    "        # Reemplazar n√∫meros no marcados\n",
    "        text = re.sub(r'\\b\\d+\\b', '<NUM>', text)\n",
    "        \n",
    "        # Restaurar n√∫meros preservados\n",
    "        text = re.sub(r'PRESERVE_(\\d+)', r'\\1', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb20085",
   "metadata": {},
   "source": [
    "# Normalizaci√≥n de May√∫sculas con Reconocimiento de Entidades\n",
    "Esta funci√≥n convierte el texto a min√∫sculas mientras preserva la capitalizaci√≥n de marcas, hashtags y entidades geopol√≠ticas. Utiliza spaCy para identificar entidades y un diccionario personalizado para marcas no reconocidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4426c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def normalize_case_with_entities(text):\n",
    "    \"\"\"\n",
    "    Normaliza may√∫sculas preservando entidades como marcas, hashtags y ubicaciones.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto a normalizar\n",
    "        \n",
    "    Returns:\n",
    "        str: Texto con may√∫sculas normalizadas\n",
    "    \"\"\"\n",
    "    # Cargar modelo de spaCy para espa√±ol\n",
    "    try:\n",
    "        nlp = spacy.load(\"es_core_news_lg\")\n",
    "    except:\n",
    "        # Si el modelo no est√° disponible, instalarlo\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"es_core_news_lg\"])\n",
    "        nlp = spacy.load(\"es_core_news_lg\")\n",
    "    \n",
    "    # Cargar diccionario personalizado de marcas (asumiendo que existe)\n",
    "    try:\n",
    "        marcas_df = pd.read_csv('marcas_comerciales.csv')\n",
    "        custom_brands = set(marcas_df['marca'].str.lower().tolist())\n",
    "    except:\n",
    "        # Si no existe el archivo, crear un conjunto vac√≠o\n",
    "        custom_brands = set()\n",
    "    \n",
    "    # Procesar el texto con spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Crear una lista para almacenar las palabras procesadas\n",
    "    processed_words = []\n",
    "    entities_spans = []\n",
    "    \n",
    "    # Identificar entidades y sus posiciones\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['ORG', 'LOC', 'PRODUCT', 'GPE', 'PERSON']:\n",
    "            entities_spans.append((ent.start_char, ent.end_char, ent.text))\n",
    "    \n",
    "    # Identificar hashtags y preservarlos\n",
    "    for match in re.finditer(r'#\\w+', text):\n",
    "        entities_spans.append((match.start(), match.end(), match.group()))\n",
    "    \n",
    "    # Convertir todo a min√∫sculas primero\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Reemplazar entidades preservando su capitalizaci√≥n original\n",
    "    result = []\n",
    "    last_end = 0\n",
    "    \n",
    "    # Ordenar entidades por posici√≥n\n",
    "    entities_spans.sort()\n",
    "    \n",
    "    for start, end, entity_text in entities_spans:\n",
    "        # A√±adir texto en min√∫sculas hasta la entidad\n",
    "        result.append(text_lower[last_end:start])\n",
    "        # A√±adir la entidad con capitalizaci√≥n original\n",
    "        result.append(entity_text)\n",
    "        last_end = end\n",
    "    \n",
    "    # A√±adir el resto del texto en min√∫sculas\n",
    "    if last_end < len(text):\n",
    "        result.append(text_lower[last_end:])\n",
    "    \n",
    "    normalized_text = ''.join(result)\n",
    "    \n",
    "    # Verificar palabras contra diccionario personalizado\n",
    "    words = re.findall(r'\\b\\w+\\b', normalized_text)\n",
    "    for word in words:\n",
    "        if word.lower() in custom_brands and word.lower() != word:\n",
    "            # Restaurar capitalizaci√≥n original de marca\n",
    "            original_case = text[text.lower().find(word.lower()):text.lower().find(word.lower()) + len(word)]\n",
    "            normalized_text = normalized_text.replace(word, original_case)\n",
    "    \n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55912c25",
   "metadata": {},
   "source": [
    "# Aplicaci√≥n de M√©todos de Procesamiento\n",
    "Esta secci√≥n carga muestras de validaci√≥n y aplica las funciones de limpieza, normalizaci√≥n de n√∫meros y unidades, y normalizaci√≥n de may√∫sculas. Los resultados se imprimen para cada muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d5d28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "Original: üî•¬°OFERTA! Compre 2x zapatos Nike a $99.99 (antes $150) üëü. ¬°V√°lido hasta el 30/11/2023! Visita https://marketmind.com/oferta-nike. Atenci√≥n @MariaP: ¬øEnv√≠o gratis? üòÉ #ModaDeportiva2023.\n",
      "--------------------------------------------------------------------------------\n",
      "After clean_business_text:\n",
      "OFERTA! Compre 2x zapatos Nike a $99.99 antes $150 V√°lido hasta el 30/11/2023! Visita Atenci√≥n Env√≠o gratis? #ModaDeportiva2023\n",
      "--------------------------------------------------------------------------------\n",
      "After normalize_numbers_and_units:\n",
      "üî•¬°OFERTA! Compre 2_unidades zapatos Nike a <USD>99.99 (antes <USD>150) üëü. ¬°V√°lido hasta el 2023-11-30! Visita https://marketmind.com/oferta-nike. Atenci√≥n @MariaP: ¬øEnv√≠o gratis? üòÉ #ModaDeportiva2023.\n",
      "--------------------------------------------------------------------------------\n",
      "After normalize_case_with_entities:\n",
      "üî•¬°oferta! compre 2x zapatos Nike a $99.99 (antes $150) üëü. ¬°v√°lido hasta el 30/11/2023! visita https://marketmind.com/oferta-nike. atenci√≥n @mariap: ¬øenv√≠o gratis? üòÉ #ModaDeportiva2023.\n",
      "================================================================================\n",
      "\n",
      "Sample 2:\n",
      "Original: I love my new iPhone 12! üòç Battery life is amazing. Bought it at 799‚Ç¨ from https://apple.com. #TechReview\n",
      "--------------------------------------------------------------------------------\n",
      "After clean_business_text:\n",
      "I love my new iPhone 12! Battery life is amazing Bought it at 799 from #TechReview\n",
      "--------------------------------------------------------------------------------\n",
      "After normalize_numbers_and_units:\n",
      "I love my new iPhone <NUM>! üòç Battery life is amazing. Bought it at <EUR>799 from https://apple.com. #TechReview\n",
      "--------------------------------------------------------------------------------\n",
      "After normalize_case_with_entities:\n",
      "i love my new iphone 12! üòç battery life is amazing. bought it at 799‚Ç¨ from https://apple.com. #TechReview\n",
      "================================================================================\n",
      "\n",
      "Sample 3:\n",
      "Original: @Juan gracias por la promo! Compr√© 3kg de caf√© a $15.00. Lleg√≥ en 24h üïí. Calidad 10/10 üëç.\n",
      "--------------------------------------------------------------------------------\n",
      "After clean_business_text:\n",
      "gracias por la promo! Compr√© 3kg de caf√© a $15.00 Lleg√≥ en 24h Calidad 10/10\n",
      "--------------------------------------------------------------------------------\n",
      "After normalize_numbers_and_units:\n",
      "@Juan gracias por la promo! Compr√© 3_kg de caf√© a <USD>15.00. Lleg√≥ en 24h üïí. Calidad 10/10 üëç.\n",
      "--------------------------------------------------------------------------------\n",
      "After normalize_case_with_entities:\n",
      "@Juan gracias por la promo! compr√© 3kg de caf√© a $15.00. lleg√≥ en 24h üïí. calidad 10/10 üëç.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load validation samples\n",
    "with open('validation_samples.json', 'r', encoding='utf-8') as file:\n",
    "    samples = json.load(file)\n",
    "\n",
    "# Select a subset of samples to process\n",
    "sample_phrases = samples[:3] if len(samples) >= 3 else samples\n",
    "\n",
    "# Function to apply all methods and display results\n",
    "def display_all_processings(text):\n",
    "    print(f\"Original: {text}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        print(\"ERROR: El texto no es una cadena. Tipo recibido:\", type(text))\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        cleaned = clean_business_text(text)\n",
    "        print(f\"After clean_business_text:\\n{cleaned}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in clean_business_text: {e}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        normalized_numbers = normalize_numbers_and_units(text)\n",
    "        print(f\"After normalize_numbers_and_units:\\n{normalized_numbers}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalize_numbers_and_units: {e}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        normalized_case = normalize_case_with_entities(text)\n",
    "        print(f\"After normalize_case_with_entities:\\n{normalized_case}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalize_case_with_entities: {e}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Process each sample phrase\n",
    "for i, phrase in enumerate(sample_phrases):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    display_all_processings(phrase['input'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
